{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A word2vec approach with `gensim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's `word2vec`?\n",
    "Roughly speaking, it's a shallow neural network model\n",
    "that can be trained to create a word embedding for NLP.\n",
    "There are two architectures:\n",
    "\n",
    "* Continuous BoW (Bag of Words),\n",
    "  this one tries to predict a word given the context;\n",
    "* Continuous skip-gram,\n",
    "  this one tries to predict the context from a given word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll use `gensim` again, it's an open source library\n",
    "that was created as part of the\n",
    "[Radim Řehůřek's Ph.D. Thesis](\n",
    "  https://radimrehurek.com/phd_rehurek.pdf\n",
    "), *Scalability of semantic analysis\n",
    "    in natural language processing*, 2011.\n",
    "His thesis is mainly towards LSA (Latent Semantic Analysis),\n",
    "and LDA (Latent Dirichlet Allocation).\n",
    "However, `word2vec` was published after that,\n",
    "by Tomas Mikolov, Kai Chen, Greg Corrado and Jeffrey Dean\n",
    "(a team of Google researchers),\n",
    "in the *Efficient Estimation of\n",
    "        Word Representations in Vector Space*, 2013\n",
    "\\[[PDF](https://arxiv.org/pdf/1301.3781.pdf),\n",
    "  [C++ code](https://code.google.com/archive/p/word2vec/)\\].\n",
    "Radim Řehůřek himself added `word2vec` to his `gensim` library,\n",
    "and published a [short tutorial for it](\n",
    "  https://rare-technologies.com/word2vec-tutorial/\n",
    ")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia trigram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a first try of `word2vec` in \\[Brazilian\\] Portuguese,\n",
    "one can see the Felipe Parpinelli's\n",
    "[word2vec-pt-br](https://github.com/felipeparpinelli/)\n",
    "repository (unfortunately, only available for Python 2).\n",
    "However, [he trained a trigram vector model and published it](\n",
    "  https://drive.google.com/file/d/0B_eXEo_eUPCDWnJ0YWtUdW1kVFk/view\n",
    "),\n",
    "so we can directly use here in Python 3.7 with `gensim`.\n",
    "The model is a 2GB file whose SHA256 is\n",
    "`5421465d49a5f709f81cec3607c64b1e6a0724fdce94f9d507a48fe07f95d098`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_model = KeyedVectors.load_word2vec_format(\"wiki.pt.trigram.vector\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has a vocabulary of more than one million words and expressions,\n",
    "all in lower case, with underscores as separators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1264918"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wiki_model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word similarity and \"maths with words\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of today, in the cited repository,\n",
    "Parpinelli is only using two model methods:\n",
    "`most_similar` and `doesnt_match`.\n",
    "The first one can be used to find similar words,\n",
    "with a similarity measurement\n",
    "ranging from $0$ to $1$.\n",
    "This example shows the name of cities\n",
    "in the São Paulo state, Brazil,\n",
    "given the name of one city:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ribeirão_preto', 0.7867798805236816),\n",
       " ('sorocaba', 0.7684873342514038),\n",
       " ('jundiaí', 0.7378007173538208),\n",
       " ('araraquara', 0.7296241521835327),\n",
       " ('são_paulo', 0.7239118814468384),\n",
       " ('guarulhos', 0.7190227508544922),\n",
       " ('bauru', 0.708629310131073),\n",
       " ('botucatu', 0.6960499882698059),\n",
       " ('taubaté', 0.6935198307037354),\n",
       " ('mogi_das_cruzes', 0.6845329403877258)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_model.most_similar(\"campinas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the words\n",
    "that have the highest similarity with `campinas`.\n",
    "Such list of tuples can be easily converted\n",
    "to a Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ribeirão_preto</th>\n",
       "      <td>0.786780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sorocaba</th>\n",
       "      <td>0.768487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jundiaí</th>\n",
       "      <td>0.737801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>araraquara</th>\n",
       "      <td>0.729624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>são_paulo</th>\n",
       "      <td>0.723912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guarulhos</th>\n",
       "      <td>0.719023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bauru</th>\n",
       "      <td>0.708629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>botucatu</th>\n",
       "      <td>0.696050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>taubaté</th>\n",
       "      <td>0.693520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mogi_das_cruzes</th>\n",
       "      <td>0.684533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 similarity\n",
       "token                      \n",
       "ribeirão_preto     0.786780\n",
       "sorocaba           0.768487\n",
       "jundiaí            0.737801\n",
       "araraquara         0.729624\n",
       "são_paulo          0.723912\n",
       "guarulhos          0.719023\n",
       "bauru              0.708629\n",
       "botucatu           0.696050\n",
       "taubaté            0.693520\n",
       "mogi_das_cruzes    0.684533"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    wiki_model.most_similar(\"campinas\"),\n",
    "    columns=[\"token\", \"similarity\"],\n",
    ").set_index(\"token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we're trying to predict the context vector from a word,\n",
    "what this gives is that\n",
    "all these words can easily appear in the same contexts.\n",
    "Though the training process of `word2vec` is unsupervised\n",
    "(it's a dimensionality reduction algorithm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of a single word,\n",
    "we can also give a list of *positive* and *negative* words,\n",
    "performing something akin to this math:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cll}\n",
    "{}   & Brasília & \\text{# federal capital of Brazil} \\\\\n",
    "{} - & Brasil   & \\text{# Brazil, in Brazilian Portuguese} \\\\\n",
    "{} + & Alemanha & \\text{# Germany, in Brazilian Portuguese} \\\\ \\hline\n",
    "{}   & ???\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('berlin', 0.5845881104469299)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_model.most_similar(\n",
    "    positive=[\"brasilia\", \"alemanha\"],\n",
    "    negative=[\"brasil\"],\n",
    "    topn=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical \"equation\" is $king - man + woman$,\n",
    "the first example in the 2013 paper,\n",
    "which here also yields $queen$\n",
    "(but with all the words in Brazilian Portuguese):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rainha', 0.6084680557250977)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_model.most_similar(\n",
    "    positive=[\"rei\", \"mulher\"], # [\"king\", \"woman\"]\n",
    "    negative=[\"homem\"],         # [\"man\"]\n",
    "    topn=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vector normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the vector regarding a word to make some actual maths with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wiki_model[\"brasilia\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_model[\"brasilia\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in this case, using the `similar_by_vector` method,\n",
    "we need to manually remove the similarity with itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('campinas', 1.0),\n",
       " ('ribeirão_preto', 0.7867798805236816),\n",
       " ('sorocaba', 0.7684873342514038),\n",
       " ('jundiaí', 0.7378007769584656),\n",
       " ('araraquara', 0.7296241521835327)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_model.similar_by_vector(wiki_model[\"campinas\"], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And performing the maths doesn't result in the same vectors,\n",
    "as not all vectors will have the same weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>magdeburg</th>\n",
       "      <td>0.542283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>erfurt</th>\n",
       "      <td>0.525711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>krefeld</th>\n",
       "      <td>0.520014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alta_baviera</th>\n",
       "      <td>0.517179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aachen</th>\n",
       "      <td>0.516245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freiburg</th>\n",
       "      <td>0.516041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baixa_saxónia</th>\n",
       "      <td>0.508358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salzburg</th>\n",
       "      <td>0.506056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ulm</th>\n",
       "      <td>0.505461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>koblenz</th>\n",
       "      <td>0.505086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               similarity\n",
       "token                    \n",
       "magdeburg        0.542283\n",
       "erfurt           0.525711\n",
       "krefeld          0.520014\n",
       "alta_baviera     0.517179\n",
       "aachen           0.516245\n",
       "freiburg         0.516041\n",
       "baixa_saxónia    0.508358\n",
       "salzburg         0.506056\n",
       "ulm              0.505461\n",
       "koblenz          0.505086"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    wiki_model.similar_by_vector(\n",
    "        wiki_model[\"brasilia\"] - wiki_model[\"brasil\"] + wiki_model[\"alemanha\"],\n",
    "        topn=10,\n",
    "    ),\n",
    "    columns=[\"token\", \"similarity\"],\n",
    ").set_index(\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rei</th>\n",
       "      <td>0.713224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rainha</th>\n",
       "      <td>0.626994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>consorte</th>\n",
       "      <td>0.553226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rainha_viúva</th>\n",
       "      <td>0.531954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mulher</th>\n",
       "      <td>0.513182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rainha_consorte</th>\n",
       "      <td>0.508050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rainha_isabel</th>\n",
       "      <td>0.507507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monarca</th>\n",
       "      <td>0.502872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>princesa</th>\n",
       "      <td>0.501628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rainha_regente</th>\n",
       "      <td>0.498720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 similarity\n",
       "token                      \n",
       "rei                0.713224\n",
       "rainha             0.626994\n",
       "consorte           0.553226\n",
       "rainha_viúva       0.531954\n",
       "mulher             0.513182\n",
       "rainha_consorte    0.508050\n",
       "rainha_isabel      0.507507\n",
       "monarca            0.502872\n",
       "princesa           0.501628\n",
       "rainha_regente     0.498720"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    wiki_model.similar_by_vector(\n",
    "        wiki_model[\"rei\"] - wiki_model[\"homem\"] + wiki_model[\"mulher\"],\n",
    "        topn=10,\n",
    "    ),\n",
    "    columns=[\"token\", \"similarity\"],\n",
    ").set_index(\"token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's because the vector magnitude is way too different,\n",
    "and we care mostly about the vector direction,\n",
    "not the vector magnitude.\n",
    "Let's calculate the vector magnitude/norm\n",
    "for each of these words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brasilia': 9.4632, 'brasil': 28.960426, 'alemanha': 24.099485}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: np.sqrt((wiki_model[k] ** 2).sum())\n",
    " for k in [\"brasilia\", \"brasil\", \"alemanha\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rei': 2.9873471, 'homem': 2.229483, 'mulher': 2.2330337}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: np.sqrt((wiki_model[k] ** 2).var())\n",
    " for k in [\"rei\", \"homem\", \"mulher\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give the same weight to these vectors,\n",
    "we need to normalize them before doing that sum/subtraction math.\n",
    "We can simply divide the vectors by the numbers above (their norm),\n",
    "but that's already done by the `word_vec` method\n",
    "when `use_norm=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(wiki_model.word_vec(\"rei\", use_norm=True) ** 2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the most similar vectors again\n",
    "(using the direction, not the magnitude):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>berlin</th>\n",
       "      <td>0.584588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hamburg</th>\n",
       "      <td>0.580028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salzburg</th>\n",
       "      <td>0.579465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>münchen</th>\n",
       "      <td>0.572176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freiburg</th>\n",
       "      <td>0.571325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sinsheim</th>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>köln</th>\n",
       "      <td>0.561137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nürnberg</th>\n",
       "      <td>0.560043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>krefeld</th>\n",
       "      <td>0.559181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frankfurt_oder</th>\n",
       "      <td>0.558645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                similarity\n",
       "token                     \n",
       "berlin            0.584588\n",
       "hamburg           0.580028\n",
       "salzburg          0.579465\n",
       "münchen           0.572176\n",
       "freiburg          0.571325\n",
       "sinsheim          0.562500\n",
       "köln              0.561137\n",
       "nürnberg          0.560043\n",
       "krefeld           0.559181\n",
       "frankfurt_oder    0.558645"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    wiki_model.similar_by_vector(\n",
    "          wiki_model.word_vec(\"brasilia\", True)\n",
    "        - wiki_model.word_vec(\"brasil\", True)\n",
    "        + wiki_model.word_vec(\"alemanha\", True),\n",
    "        topn=10,\n",
    "    ),\n",
    "    columns=[\"token\", \"similarity\"],\n",
    ").set_index(\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rei</th>\n",
       "      <td>0.656337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rainha</th>\n",
       "      <td>0.608468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>consorte</th>\n",
       "      <td>0.547408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mulher</th>\n",
       "      <td>0.534614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rainha_viúva</th>\n",
       "      <td>0.525085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>esposa</th>\n",
       "      <td>0.499288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rainha_consorte</th>\n",
       "      <td>0.498275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>princesa</th>\n",
       "      <td>0.494415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rainha_isabel</th>\n",
       "      <td>0.493366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rainha_regente</th>\n",
       "      <td>0.490066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 similarity\n",
       "token                      \n",
       "rei                0.656337\n",
       "rainha             0.608468\n",
       "consorte           0.547408\n",
       "mulher             0.534614\n",
       "rainha_viúva       0.525085\n",
       "esposa             0.499288\n",
       "rainha_consorte    0.498275\n",
       "princesa           0.494415\n",
       "rainha_isabel      0.493366\n",
       "rainha_regente     0.490066"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    wiki_model.similar_by_vector(\n",
    "          wiki_model.word_vec(\"rei\", True)\n",
    "        - wiki_model.word_vec(\"homem\", True)\n",
    "        + wiki_model.word_vec(\"mulher\", True),\n",
    "        topn=10,\n",
    "    ),\n",
    "    columns=[\"token\", \"similarity\"],\n",
    ").set_index(\"token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's what the `most_similar` method does,\n",
    "and it's also what the original *word2vec* implementation does,\n",
    "as the method documentation states:\n",
    "\n",
    "> The method corresponds to the `word-analogy` and `distance` scripts\n",
    "> in the original word2vec implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an explanation of the maths,\n",
    "we're looking for a vector $b^*$ that maximizes $b^* \\cdot (b - a + a^*)$,\n",
    "working with unit-length vectors $b$, $a$ and $a^*$\n",
    "that corresponds to the input words.\n",
    "That can also be written as a maximization of\n",
    "$b^* \\cdot b - b^* \\cdot a + b^* \\cdot a^*$,\n",
    "or, defining a binary operator $\\cos$ as the cosine similarity\n",
    "(dot product divided by the norms of each vector), we can write it as:\n",
    "\n",
    "$$\\arg \\max_{b^*} \\cos(b^*, b) - \\cos(b^*, a) + \\cos(b^*, a^*)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative similarity formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Omer Levy and Yoav Goldberg, in\n",
    "*Linguistic Regularities in Sparse and Explicit Word Representations*, 2014\n",
    "\\[[PDF](http://www.aclweb.org/anthology/W14-1618)\\],\n",
    "named as *3CosAdd* the similarity algorithm we've just seen,\n",
    "and they proposed an alternative \"multiplicative\" algorithm called *3CosMul*,\n",
    "that reminds us of applying the c of each addend/subtrahend\n",
    "in the \"sum/subtraction of cosine similarities\" function we were maximizing.\n",
    "This new equation can also be seen as something like\n",
    "taking a geometric mean of the terms, instead of a simple average.\n",
    "\n",
    "$$\n",
    "\\arg \\max_{b^*} \\dfrac{\\cos(b^*, b) \\cos(b^*, a^*)}\n",
    "                      {\\cos(b^*, a) + \\varepsilon}\n",
    "$$\n",
    "\n",
    "That's exactly what the `most_similar_cosmul` method does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('frankfurt_oder', 0.9992457628250122),\n",
       " ('hamburg', 0.9958572387695312),\n",
       " ('salzburg', 0.993118941783905),\n",
       " ('magdeburg', 0.979534387588501),\n",
       " ('krefeld', 0.9781520962715149),\n",
       " ('oberhausen', 0.9737989902496338),\n",
       " ('zürich', 0.9721503853797913),\n",
       " ('budapest', 0.9673693776130676),\n",
       " ('berlin', 0.9658992290496826),\n",
       " ('vienna', 0.9648484587669373)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_model.most_similar_cosmul(\n",
    "    positive=[\"brasilia\", \"alemanha\"],\n",
    "    negative=[\"brasil\"],\n",
    "    topn=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rainha', 0.9783447980880737),\n",
       " ('consorte', 0.9311118721961975),\n",
       " ('rainha_viúva', 0.9254377484321594),\n",
       " ('rainha_consorte', 0.9043235778808594),\n",
       " ('rainha_isabel', 0.8911893963813782),\n",
       " ('rainha_regente', 0.8879780173301697),\n",
       " ('infanta', 0.8857831358909607),\n",
       " ('princesa', 0.8801829814910889),\n",
       " ('rainha_reinante', 0.8782250881195068),\n",
       " ('concubina', 0.8746837377548218)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_model.most_similar_cosmul(\n",
    "    positive=[\"rei\", \"mulher\"],\n",
    "    negative=[\"homem\"],\n",
    "    topn=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It kept *rainha* (queen) as the most similar word\n",
    "in the $king - man + woman$ equation,\n",
    "but *berlin* (Berlin) is no longer the most similar word\n",
    "in the $Brasília - Brazil + Germany$ equation.\n",
    "The similarity values are way higher,\n",
    "but Vienna is the federal capital of Austria,\n",
    "it's no longer a city in Germany.\n",
    "In some sense, these $2$ cherry-picked examples\n",
    "aren't better in *3CosMul* than in *3CosAdd*,\n",
    "that suffices for us to avoid dropping any of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lack of data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've found that some words aren't properly cleaned in the input,\n",
    "and that might rule out what we would expect\n",
    "with \"words maths\" like the ones previously performed here.\n",
    "An example is the word *frequency*,\n",
    "which in portuguese is *frequência*,\n",
    "yet it appear in the dataset with some different writings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('frequência', 1.0),\n",
       " ('freqüência', 0.8619202971458435),\n",
       " ('frequencia', 0.6580279469490051),\n",
       " ('amplitude', 0.6013369560241699),\n",
       " ('frequências', 0.6001715660095215)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_model.similar_by_vector(wiki_model[\"frequência\"], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $frequency - hertz + seconds$ equation\n",
    "is expected to return something like *period*, *duration* or *time*,\n",
    "but none of these appears as the $5$ most similar words/expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cinco_minutos', 0.5157788991928101),\n",
       " ('minutos', 0.5141439437866211),\n",
       " ('freqüência', 0.506064772605896),\n",
       " ('dez_minutos', 0.47891607880592346),\n",
       " ('cada_vez', 0.47068893909454346)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_model.most_similar(\n",
    "    positive=[\"frequência\", \"segundos\"],\n",
    "    negative=[\"hertz\"],\n",
    "    topn=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translation of each result, in order:\n",
    "\n",
    "- Five minutes\n",
    "- Minutes\n",
    "- Frequency (another writing of the same word)\n",
    "- Ten minutes\n",
    "- Each time/turn/cycle\n",
    "  (*vez* have nothing to do with the physical meaning of *time*)\n",
    "\n",
    "The fifth entry have something to do with the idea of a cycle,\n",
    "but the word *frequency* appeared again,\n",
    "and it's strange to see these time durations in minutes\n",
    "as distinct tokens.\n",
    "Using *3CosMul* doesn't help that much,\n",
    "its new entries are *do* (of) and *já* (already / right now):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('do', 0.9138926267623901),\n",
       " ('já', 0.9123607873916626),\n",
       " ('cinco_minutos', 0.9002106785774231),\n",
       " ('cada_vez', 0.890281081199646),\n",
       " ('minutos', 0.8871634006500244)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_model.most_similar_cosmul(\n",
    "    positive=[\"frequência\", \"segundos\"],\n",
    "    negative=[\"hertz\"],\n",
    "    topn=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An expected result was *período*,\n",
    "but there are entries\n",
    "\n",
    "- Missing the acute accent\n",
    "- A typo (perído)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('período', 1.0),\n",
       " ('periodo', 0.7241699695587158),\n",
       " ('perído', 0.6035453081130981),\n",
       " ('longo_período', 0.5876239538192749),\n",
       " ('períodos', 0.5766887068748474)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_model.similar_by_vector(wiki_model[\"período\"], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these are obviously not the only words\n",
    "that hadn't been properly cleaned.\n",
    "Perhaps the result wasn't as expected\n",
    "in the last \"word math\" calculation\n",
    "because of this \"word splitting\",\n",
    "though there's no reason to believe\n",
    "that these alternative writings have some systematic bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('periodo', 1.0000001192092896),\n",
       " ('período', 0.7241699695587158),\n",
       " ('perído', 0.7190518379211426),\n",
       " ('conturbado_período', 0.6516879200935364),\n",
       " ('interregno', 0.6100722551345825)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_model.similar_by_vector(wiki_model[\"periodo\"], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a `word2vec` model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could a `word2vec` model replace an LSI model\n",
    "to create a word embedding\n",
    "that would be helpful on\n",
    "either finding invalid/inconsistent entries\n",
    "or filling missing values\n",
    "on a dataset of document affiliations?\n",
    "\n",
    "Let's perform the same procedure applied on LSI in the previous experiment,\n",
    "but, this time, let's use it to train a word2vec model.\n",
    "\n",
    "The goal is to:\n",
    "\n",
    "- Fit a `word2vec` model with the CSV created on 2018-06-04,\n",
    "  ignoring the country fields;\n",
    "- Fit a random forest model to detect the country from the `word2vec` vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple/small example data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `gensim` word list example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "common_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model is quite straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "simple_model = Word2Vec(common_texts,\n",
    "    size=100,\n",
    "    window=5,\n",
    "    min_count=1, # Don't ignore rare words\n",
    "    sg=1, # Skip-gram architecture\n",
    "    workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedded vectors are stored as a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.00429325, -0.00053441, -0.00069498, ..., -0.00280878,\n",
       "        -0.00281088,  0.00136091],\n",
       "       [ 0.00264861, -0.00271103, -0.00236115, ..., -0.00461583,\n",
       "         0.00246741, -0.00283012],\n",
       "       [-0.00305232,  0.00020281, -0.00423114, ..., -0.00494703,\n",
       "        -0.00124028,  0.00197053],\n",
       "       ...,\n",
       "       [ 0.0021847 , -0.00360832, -0.00217316, ..., -0.0048031 ,\n",
       "        -0.00139142, -0.00275932],\n",
       "       [-0.0002061 ,  0.00336424, -0.00263451, ...,  0.00407349,\n",
       "         0.00052204, -0.00373554],\n",
       "       [ 0.00118459,  0.00014319, -0.00420273, ..., -0.00492786,\n",
       "         0.00173646,  0.00473093]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(simple_model.wv.vectors.shape)\n",
    "simple_model.wv.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words to which the each row belongs are stored in the `index2word` list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['system',\n",
       " 'user',\n",
       " 'trees',\n",
       " 'graph',\n",
       " 'human',\n",
       " 'interface',\n",
       " 'computer',\n",
       " 'survey',\n",
       " 'response',\n",
       " 'time',\n",
       " 'eps',\n",
       " 'minors']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model.wv.index2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(simple_model.wv.vectors ==\n",
    "       np.array([simple_model.wv[word]\n",
    "                 for idx, word in enumerate(simple_model.wv.index2word)])\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That information should be enough\n",
    "for us to create a model with the actual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the word lists and training `word2vec` w/ non-country text fields from Clea's CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll work with the same dataset\n",
    "(coming from Clea's output, created on 2018-06-04)\n",
    "as the previous regarding LSI/LSA (it started on 2018-08-23):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"inner_join_2018-06-04.csv\",\n",
    "                      dtype=str,\n",
    "                      keep_default_na=False) \\\n",
    "            .drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first cleaning step is the same one from that experiment,\n",
    "which removes accents (keeping the letters), lowercases everything\n",
    "and removes any other character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "TEXT_ONLY_REGEX = re.compile(\"[^a-zA-Z ]\")\n",
    "\n",
    "def pre_normalize(name):\n",
    "    return TEXT_ONLY_REGEX.sub(\"\", unidecode(name).lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same stopwords as before, we can create a function\n",
    "that gets a dataset and casts it as a word list.\n",
    "Here we don't need to worry about uncommon (single occurrence) words,\n",
    "that's a `word2vec` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"da\", \"de\", \"desta\", \"do\", \"em\", \"ii\", \"iii\", \"in\", \"mesma\", \"no\", \"pela\", \"pelos\"]\n",
    "\n",
    "def df2wlist(dset, stop_words=stop_words):\n",
    "    return dset.T.apply(\n",
    "        lambda row: [\n",
    "            word for word in pre_normalize(\" \".join(row)).split()\n",
    "                 if word not in stop_words\n",
    "                 and len(word) > 1\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following entries are all the field names from that CSV\n",
    "with some content that might be useful for us\n",
    "from which we want to find the `addr_country_code`.\n",
    "When training with a joined-columns word list,\n",
    "the order of the fields matters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fields = [\n",
    "    \"addr_city\",\n",
    "    \"addr_state\",\n",
    "    \"aff_text\",\n",
    "    \"article_title\",\n",
    "    \"contrib_bio\",\n",
    "    \"contrib_prefix\",\n",
    "    \"contrib_name\",\n",
    "    \"contrib_surname\",\n",
    "    \"institution_orgdiv1\",\n",
    "    \"institution_orgdiv2\",\n",
    "    \"institution_orgname\",\n",
    "    \"institution_orgname_rewritten\",\n",
    "    \"institution_original\",\n",
    "    \"institution_orgname_rewritten\",\n",
    "    \"journal_title\",\n",
    "    \"publisher_name\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can get the desired wordlist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.6 s, sys: 118 ms, total: 14.7 s\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wlist = df2wlist(dataset[x_fields])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94655    [universidade, estado, rio, janeiro, instituto...\n",
       "94656    [ciudad, buenos, aires, ciudad, buenos, aires,...\n",
       "94657    [joao, pessoa, pb, universidade, federal, para...\n",
       "94658    [sao, paulo, universidade, sao, paulo, departa...\n",
       "94659    [campinas, sp, universidade, estadual, campina...\n",
       "dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wlist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 48s, sys: 460 ms, total: 5min 49s\n",
      "Wall time: 1min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Word2Vec(wlist,\n",
    "    size=200,\n",
    "    window=7,\n",
    "    min_count=2, # Ignore uncommon words\n",
    "    sg=1, # Skip-gram architecture\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was really fast!\n",
    "\n",
    "Although the `word2vec` neural network model is shallow,\n",
    "the data we've got here might be too small.\n",
    "Anyway, we know how to compare words,\n",
    "but we still doesn't know how to compare documents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
